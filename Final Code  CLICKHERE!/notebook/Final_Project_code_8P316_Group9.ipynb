{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow, imread\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "from residual_blocks import identity_block_small, identity_block_large, convolutional_block_small, convolutional_block_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the small or large models\n",
    "Run either build_model_small for model with small building block or build_model_large for model with large building blocks,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 3)\n",
    "classes = 1\n",
    "\n",
    "def build_model_small(hp):\n",
    "    \"\"\"\n",
    "    Implementation of a ResNet model with varying amount of stages and id blocks using small residual blocks\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=he_normal(seed=None))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_small(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block_small(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block_small(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    #Y = 20 #for 1 additional stage, \n",
    "    #Y = 18 #for 2 additional stages \n",
    "    #Y = 16 #for 3 additional stages\n",
    "    \n",
    "    # Stage 3 = first additional stage\n",
    "    X = convolutional_block_small(X, f=3, filters=[128,128,512], stage=3, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks1', 1, Y)): \n",
    "        X = identity_block_small(X, 3, [128,128,512], stage=3, block=f'_{i}_')\n",
    "\n",
    "    # Stage 4 = second additional stage (optional, comment when not used)\n",
    "    X = convolutional_block_small(X, f=3, filters=[256,256,1024], stage=4, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks2', 1, Y)):\n",
    "        X = identity_block_small(X, 3, [256,256,1024], stage=4, block=f'_{i}_')\n",
    "    \n",
    "    # Stage 5 = third additional stage (optional, comment when not used)\n",
    "    X = convolutional_block_small(X, f=3, filters=[512,512,2084], stage=5, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks3', 1, Y)):\n",
    "        X = identity_block_small(X, 3, [512,512,2084], stage=5, block=f'_{i}_')\n",
    "    \n",
    "    \n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\" (comment when using stage 5)\n",
    "    X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = he_normal(seed=None))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    model.compile(SGD(lr=1e-2, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 3)\n",
    "classes = 1\n",
    "\n",
    "def build_model_large(hp):\n",
    "    \"\"\"\n",
    "    Implementation of a ResNet model with varying amount of stages and id blocks using large residual blocks\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=he_normal(seed=None))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block_large(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block_large(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block_large(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    Y = 62 #for 1 additional stage, \n",
    "    #Y = 31 #for 2 additional stages \n",
    "    #Y = 20 #for 3 additional stages\n",
    "    \n",
    "    # Stage 3 = first additional stage\n",
    "    X = convolutional_block_large(X, f=3, filters=[128,128,512], stage=3, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks1', 1, Y)): \n",
    "        X = identity_block_large(X, 3, [128,128,512], stage=3, block=f'_{i}_')\n",
    "\n",
    "    # Stage 4 = second additional stage (optional, comment when not used)\n",
    "    X = convolutional_block_large(X, f=3, filters=[256,256,1024], stage=4, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks2', 1, Y)):\n",
    "        X = identity_block_large(X, 3, [256,256,1024], stage=4, block=f'_{i}_')\n",
    "    \n",
    "    # Stage 5 = third additional stage (optional, comment when not used)\n",
    "    X = convolutional_block_large(X, f=3, filters=[512,512,2084], stage=5, block='a', s=2)\n",
    "    for i in range(hp.Int('n_IDblocks3', 1, Y)):\n",
    "        X = identity_block_large(X, 3, [512,512,2084], stage=5, block=f'_{i}_')\n",
    "    \n",
    "    \n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\" (comment when using stage 5)\n",
    "    #X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='sigmoid', name='fc' + str(classes), kernel_initializer = he_normal(seed=None))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    model.compile(SGD(lr=1e-2, momentum=0.95), loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### insert hyperparameters ################\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "###################################################\n",
    "\n",
    "# Change these variables to point at the locations and names of the test dataset and your models!!\n",
    "base_dir = r'C:\\Users\\20153761\\Documents\\TUe\\4e jaar\\3e kwartiel\\BIA\\train+val'\n",
    "test_dir = r'C:\\Users\\20153761\\Documents\\TUe\\4e jaar\\3e kwartiel\\BIA\\test\\test'\n",
    "\n",
    "# dataset parameters\n",
    "TRAIN_PATH = os.path.join(base_dir, 'train')\n",
    "VALID_PATH = os.path.join(base_dir, 'valid')\n",
    "RESCALING_FACTOR = 1./255\n",
    "IMAGE_SIZE = 96\n",
    "\n",
    "# instantiate data generators\n",
    "datagen = ImageDataGenerator(rescale=RESCALING_FACTOR)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                    batch_size=train_batch_size,\n",
    "                                    class_mode='binary',\n",
    "                                    shuffle=True)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(VALID_PATH,\n",
    "                                    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                    batch_size=val_batch_size,\n",
    "                                    class_mode='binary',\n",
    "                                    shuffle=False)\n",
    "\n",
    "# form steps\n",
    "train_steps = train_gen.n//train_gen.batch_size\n",
    "val_steps = val_gen.n//val_gen.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KerasTuner for hyperparameter optimalisation using hyperband "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory and project_name should be changed for different models\n",
    "tuner = Hyperband(\n",
    "    build_model_large, # choose between build_model_small or build_model_large\n",
    "    overwrite=True,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs = 15,\n",
    "    factor = 2,\n",
    "    directory ='storage/ResnetTuner_L',\n",
    "    project_name='ResNet_Stage12345_L'\n",
    ")\n",
    "\n",
    "# stop early to shorten tuning duration\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# search for optimal hyperparameters\n",
    "tuner.search(train_gen,\n",
    "             verbose=1, \n",
    "             epochs=25,\n",
    "             batch_size=32,\n",
    "             steps_per_epoch=train_steps,\n",
    "             callbacks=[stop_early],\n",
    "             validation_steps=val_steps,\n",
    "             validation_data=val_gen)\n",
    "\n",
    "# get best hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model and weights (change names!)\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model_name = 'ResNet_Stage12345_L'\n",
    "model_filepath = './' + 'storage' + '/' + model_name + '.json'\n",
    "weights_filepath = './' + 'storage' + '/' + model_name + '_weights.hdf5'\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_filepath, 'w') as json_file:\n",
    "    json_file.write(model_json) \n",
    "    \n",
    "# define the model checkpoint and Tensorboard callbacks\n",
    "checkpoint = ModelCheckpoint(weights_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "tensorboard = TensorBoard(os.path.join('logs', model_name))\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# fit model\n",
    "history = model.fit(train_gen, \n",
    "          epochs = 25, \n",
    "          batch_size = 32, \n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data=val_gen,\n",
    "          validation_steps=val_steps,\n",
    "          callbacks=callbacks_list)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save submission.csv for kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and model weights\n",
    "json_file = open(model_filepath, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "# load weights into new model\n",
    "model.load_weights(weights_filepath)\n",
    "\n",
    "\n",
    "# open the test set in batches (as it is a very big dataset) and make predictions\n",
    "test_files = glob.glob(test_dir + '*')\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "file_batch = 5000\n",
    "max_idx = len(test_files)\n",
    "\n",
    "for idx in range(0, max_idx, file_batch):\n",
    "\n",
    "    print('Indexes: %i - %i'%(idx, idx+file_batch))\n",
    "\n",
    "    test_df = pd.DataFrame({'path': test_files[idx:idx+file_batch]})\n",
    "\n",
    "\n",
    "    # get the image id \n",
    "    test_df['id'] = test_df.path.map(lambda x: x.split(os.sep)[-1].split('.')[0])\n",
    "    test_df['image'] = test_df['path'].map(imread)\n",
    "    \n",
    "    \n",
    "    K_test = np.stack(test_df['image'].values)\n",
    "    \n",
    "    # apply the same preprocessing as during draining\n",
    "    K_test = K_test.astype('float')/255.0\n",
    "    \n",
    "    predictions = model.predict(K_test)\n",
    "    \n",
    "    test_df['label'] = predictions\n",
    "    submission = pd.concat([submission, test_df[['id', 'label']]])\n",
    "\n",
    "\n",
    "# save your submission (change name!)\n",
    "submission.head()\n",
    "submission.to_csv('submission_ResNet_Stage12345_L.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start TensorBoard by calling the following commands from the Anaconda Prompt command line:\n",
    "\n",
    "````bash\n",
    "activate 8p361\n",
    "cd 'path/where/logs/are'\n",
    "tensorboard --logdir logs\n",
    "````\n",
    "\n",
    "Navigate browser to http://localhost:6006/ to visualize the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
